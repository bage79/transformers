{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to generate text: using different decoding methods for language generation with Transformers\n",
    "https://huggingface.co/blog/how-to-generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q git+https://github.com/huggingface/transformers.git\n",
    "!pip install -q tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-01 09:26:33.689306: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-01-01 09:26:33.689349: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import TFGPT2LMHeadModel, GPT2Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-01 09:26:44.410421: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-01-01 09:26:44.410489: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory\n",
      "2022-01-01 09:26:44.410523: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory\n",
      "2022-01-01 09:26:44.410554: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory\n",
      "2022-01-01 09:26:44.410583: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory\n",
      "2022-01-01 09:26:44.410612: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory\n",
      "2022-01-01 09:26:44.410641: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory\n",
      "2022-01-01 09:26:44.410671: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2022-01-01 09:26:44.410681: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2022-01-01 09:26:44.411478: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-01-01 09:26:44.569807: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at gpt2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "model = TFGPT2LMHeadModel.from_pretrained(\"gpt2\", pad_token_id=tokenizer.eos_token_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Greedy Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "I enjoy walking with my cute dog, but I'm not sure if I'll ever be able to walk with my dog. I'm not sure if I'll ever be able to walk with my dog.\n",
      "\n",
      "I'm not sure if I'll\n"
     ]
    }
   ],
   "source": [
    "input_ids = tokenizer.encode(\"I enjoy walking with my cute dog\", return_tensors=\"tf\")\n",
    "output = model.generate(input_ids, max_length=50)\n",
    "print(\"output:\\n\" + 100*\"-\")\n",
    "print(tokenizer.decode(output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beam Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "I enjoy walking with my cute dog, but I'm not sure if I'll ever be able to walk with him again.\n",
      "\n",
      "I'm not sure if I'll ever be able to walk with him again. I'm not sure if I'll\n"
     ]
    }
   ],
   "source": [
    "output = model.generate(\n",
    "    input_ids, \n",
    "    max_length=50,\n",
    "    num_beams=5,\n",
    "    early_stopping=True\n",
    ")\n",
    "print(\"output:\\n\" + 100*\"-\")\n",
    "print(tokenizer.decode(output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beam Search with repetition penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "I enjoy walking with my cute dog, but I'm not sure if I'll ever be able to walk with him again.\n",
      "\n",
      "I've been thinking about this for a while now, and I think it's time for me to take a break\n"
     ]
    }
   ],
   "source": [
    "output = model.generate(\n",
    "    input_ids,\n",
    "    max_length=50,\n",
    "    num_beams=5,\n",
    "    no_repeat_ngram_size=2,\n",
    "    early_stopping=True\n",
    ")\n",
    "print(\"output:\\n\" + 100*\"-\")\n",
    "print(tokenizer.decode(output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0: I enjoy walking with my cute dog, but I'm not sure if I'll ever be able to walk with him again.\n",
      "\n",
      "I've been thinking about this for a while now, and I think it's time for me to take a break\n",
      "1: I enjoy walking with my cute dog, but I'm not sure if I'll ever be able to walk with him again.\n",
      "\n",
      "I've been thinking about this for a while now, and I think it's time for me to get back to\n",
      "2: I enjoy walking with my cute dog, but I'm not sure if I'll ever be able to walk with her again.\n",
      "\n",
      "I've been thinking about this for a while now, and I think it's time for me to take a break\n",
      "3: I enjoy walking with my cute dog, but I'm not sure if I'll ever be able to walk with her again.\n",
      "\n",
      "I've been thinking about this for a while now, and I think it's time for me to get back to\n",
      "4: I enjoy walking with my cute dog, but I'm not sure if I'll ever be able to walk with him again.\n",
      "\n",
      "I've been thinking about this for a while now, and I think it's time for me to take a step\n"
     ]
    }
   ],
   "source": [
    "output = model.generate(\n",
    "    input_ids, \n",
    "    max_length=50, \n",
    "    num_beams=5, \n",
    "    no_repeat_ngram_size=2, \n",
    "    num_return_sequences=5, \n",
    "    early_stopping=True\n",
    ")\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "for i, beam_output in enumerate(output):\n",
    "  print(\"{}: {}\".format(i, tokenizer.decode(beam_output, skip_special_tokens=True)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "I enjoy walking with my cute dog. He just gave me a whole new hand sense.\"\n",
      "\n",
      "But it seems that the dogs have learned a lot from teasing at the local batte harness once they take on the outside.\n",
      "\n",
      "\"I take\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(0)\n",
    "output = model.generate(\n",
    "    input_ids,\n",
    "    do_sample=True,\n",
    "    max_length=50,\n",
    "    top_k=0\n",
    ")\n",
    "\n",
    "print(\"output:\\n\" + 100*\"-\")\n",
    "print(tokenizer.decode(output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "I enjoy walking with my cute dog, but I don't like to be at home too much. I also find it a bit weird when I'm out shopping. I am always away from my house a lot, but I do have a few friends\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(0)\n",
    "sample_output = model.generate(\n",
    "    input_ids, \n",
    "    do_sample=True, \n",
    "    max_length=50, \n",
    "    top_k=0, \n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(sample_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top-K Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "I enjoy walking with my cute dog. It's so good to have an environment where your dog is available to share with you and we'll be taking care of you.\n",
      "\n",
      "We hope you'll find this story interesting!\n",
      "\n",
      "I am from\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(0)\n",
    "\n",
    "sample_output = model.generate(\n",
    "    input_ids, \n",
    "    do_sample=True, \n",
    "    max_length=50, \n",
    "    top_k=50\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(sample_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top-p (nucleus) sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "I enjoy walking with my cute dog. He will never be the same. I watch him play.\n",
      "\n",
      "\n",
      "Guys, my dog needs a name. Especially if he is found with wings.\n",
      "\n",
      "\n",
      "What was that? I had a lot of\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(0)\n",
    "\n",
    "sample_output = model.generate(\n",
    "    input_ids, \n",
    "    do_sample=True, \n",
    "    max_length=50, \n",
    "    top_p=0.92, \n",
    "    top_k=0\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(sample_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0: I enjoy walking with my cute dog. It's so good to have the chance to walk with a dog. But I have this problem with the dog and how he's always looking at us and always trying to make me see that I can do something\n",
      "1: I enjoy walking with my cute dog, she loves taking trips to different places on the planet, even in the desert! The world isn't big enough for us to travel by the bus with our beloved pup, but that's where I find my love\n",
      "2: I enjoy walking with my cute dog and playing with our kids,\" said David J. Smith, director of the Humane Society of the US.\n",
      "\n",
      "\"So as a result, I've got more work in my time,\" he said.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(0)\n",
    "\n",
    "# set top_k = 50 and set top_p = 0.95 and num_return_sequences = 3\n",
    "sample_outputs = model.generate(\n",
    "    input_ids,\n",
    "    do_sample=True, \n",
    "    max_length=50, \n",
    "    top_k=50, \n",
    "    top_p=0.95, \n",
    "    num_return_sequences=3\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "for i, sample_output in enumerate(sample_outputs):\n",
    "  print(\"{}: {}\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mSignature:\u001b[0m\n",
      "\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mmin_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mdo_sample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mearly_stopping\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mnum_beams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mtemperature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mtop_k\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mtop_p\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mrepetition_penalty\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mbad_words_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mbos_token_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mpad_token_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0meos_token_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mlength_penalty\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mno_repeat_ngram_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mnum_return_sequences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mdecoder_start_token_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0muse_cache\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0moutput_scores\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mreturn_dict_in_generate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mforced_bos_token_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mforced_eos_token_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m**\u001b[0m\u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeneration_tf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTFGreedySearchEncoderDecoderOutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeneration_tf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTFGreedySearchDecoderOnlyOutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeneration_tf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTFSampleEncoderDecoderOutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeneration_tf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTFSampleDecoderOnlyOutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeneration_tf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTFBeamSearchEncoderDecoderOutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeneration_tf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTFBeamSearchDecoderOnlyOutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeneration_tf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTFBeamSampleEncoderDecoderOutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeneration_tf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTFBeamSampleDecoderOnlyOutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m\n",
      "Generates sequences for models with a language modeling head. The method currently supports greedy decoding,\n",
      "beam-search decoding, sampling with temperature, sampling with top-k or nucleus sampling.\n",
      "\n",
      "Adapted in part from [Facebook's XLM beam search\n",
      "code](https://github.com/facebookresearch/XLM/blob/9e6f6814d17be4fe5b15f2e6c43eb2b2d76daeb4/src/model/transformer.py#L529).\n",
      "\n",
      "Apart from `input_ids` and `attention_mask`, all the arguments below will default to the value of the attribute\n",
      "of the same name inside the [`PretrainedConfig`] of the model. The default values indicated are the default\n",
      "values of those config.\n",
      "\n",
      "Most of these parameters are explained in more detail in [this blog\n",
      "post](https://huggingface.co/blog/how-to-generate).\n",
      "\n",
      "Parameters:\n",
      "\n",
      "    input_ids (`tf.Tensor` of `dtype=tf.int32` and shape `(batch_size, sequence_length)`, *optional*):\n",
      "        The sequence used as a prompt for the generation. If `None` the method initializes it with\n",
      "        `bos_token_id` and a batch size of 1.\n",
      "    max_length (`int`, *optional*, defaults to 20):\n",
      "        The maximum length of the sequence to be generated.\n",
      "    min_length (`int`, *optional*, defaults to 10):\n",
      "        The minimum length of the sequence to be generated.\n",
      "    do_sample (`bool`, *optional*, defaults to `False`):\n",
      "        Whether or not to use sampling ; use greedy decoding otherwise.\n",
      "    early_stopping (`bool`, *optional*, defaults to `False`):\n",
      "        Whether to stop the beam search when at least `num_beams` sentences are finished per batch or not.\n",
      "    num_beams (`int`, *optional*, defaults to 1):\n",
      "        Number of beams for beam search. 1 means no beam search.\n",
      "    temperature (`float`, *optional*, defaults to 1.0):\n",
      "        The value used to module the next token probabilities.\n",
      "    top_k (`int`, *optional*, defaults to 50):\n",
      "        The number of highest probability vocabulary tokens to keep for top-k-filtering.\n",
      "    top_p (`float`, *optional*, defaults to 1.0):\n",
      "        If set to float < 1, only the most probable tokens with probabilities that add up to `top_p` or higher\n",
      "        are kept for generation.\n",
      "    repetition_penalty (`float`, *optional*, defaults to 1.0):\n",
      "        The parameter for repetition penalty. 1.0 means no penalty. See [this\n",
      "        paper](https://arxiv.org/pdf/1909.05858.pdf) for more details.\n",
      "    pad_token_id (`int`, *optional*):\n",
      "        The id of the *padding* token.\n",
      "    bos_token_id (`int`, *optional*):\n",
      "        The id of the *beginning-of-sequence* token.\n",
      "    eos_token_id (`int`, *optional*):\n",
      "        The id of the *end-of-sequence* token.\n",
      "    length_penalty (`float`, *optional*, defaults to 1.0):\n",
      "        Exponential penalty to the length. 1.0 means no penalty.\n",
      "\n",
      "        Set to values < 1.0 in order to encourage the model to generate shorter sequences, to a value > 1.0 in\n",
      "        order to encourage the model to produce longer sequences.\n",
      "    no_repeat_ngram_size (`int`, *optional*, defaults to 0):\n",
      "        If set to int > 0, all ngrams of that size can only occur once.\n",
      "    bad_words_ids(`List[int]`, *optional*):\n",
      "        List of token ids that are not allowed to be generated. In order to get the tokens of the words that\n",
      "        should not appear in the generated text, use `tokenizer.encode(bad_word, add_prefix_space=True)`.\n",
      "    num_return_sequences(`int`, *optional*, defaults to 1):\n",
      "        The number of independently computed returned sequences for each element in the batch.\n",
      "    attention_mask (`tf.Tensor` of `dtype=tf.int32` and shape `(batch_size, sequence_length)`, *optional*):\n",
      "        Mask to avoid performing attention on padding token indices. Mask values are in `[0, 1]`, 1 for tokens\n",
      "        that are not masked, and 0 for masked tokens.\n",
      "\n",
      "        If not provided, will default to a tensor the same shape as `input_ids` that masks the pad token.\n",
      "\n",
      "        [What are attention masks?](../glossary#attention-mask)\n",
      "    decoder_start_token_id (`int`, *optional*):\n",
      "        If an encoder-decoder model starts decoding with a different token than *bos*, the id of that token.\n",
      "    use_cache: (`bool`, *optional*, defaults to `True`):\n",
      "        Whether or not the model should use the past last key/values attentions (if applicable to the model) to\n",
      "        speed up decoding.\n",
      "    output_attentions (`bool`, *optional*, defaults to `False`):\n",
      "        Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n",
      "        returned tensors for more details.\n",
      "    output_hidden_states (`bool`, *optional*, defaults to `False`):\n",
      "        Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors\n",
      "        for more details.\n",
      "    output_scores (`bool`, *optional*, defaults to `False`):\n",
      "        Whether or not to return the prediction scores. See `scores` under returned tensors for more details.\n",
      "    return_dict_in_generate (`bool`, *optional*, defaults to `False`):\n",
      "        Whether or not to return a [`~file_utils.ModelOutput`] instead of a plain tuple.\n",
      "    forced_bos_token_id (`int`, *optional*):\n",
      "        The id of the token to force as the first generated token after the `decoder_start_token_id`. Useful\n",
      "        for multilingual models like [mBART](../model_doc/mbart) where the first generated token needs to be\n",
      "        the target language token.\n",
      "    forced_eos_token_id (`int`, *optional*):\n",
      "        The id of the token to force as the last generated token when `max_length` is reached.\n",
      "    model_specific_kwargs:\n",
      "        Additional model specific kwargs will be forwarded to the `forward` function of the model.\n",
      "\n",
      "Return:\n",
      "    [`~file_utils.ModelOutput`] or `tf.Tensor`: A [`~file_utils.ModelOutput`] (if\n",
      "    `return_dict_in_generate=True` or when `config.return_dict_in_generate=True`) or a `tf.Tensor`.\n",
      "\n",
      "        If the model is *not* an encoder-decoder model (`model.config.is_encoder_decoder=False`), the possible\n",
      "        [`~file_utils.ModelOutput`] types are:\n",
      "\n",
      "            - [`~generation_utils.TFGreedySearchDecoderOnlyOutput`],\n",
      "            - [`~generation_utils.TFSampleDecoderOnlyOutput`],\n",
      "            - [`~generation_utils.TFBeamSearchDecoderOnlyOutput`],\n",
      "            - [`~generation_utils.TFBeamSampleDecoderOnlyOutput`]\n",
      "\n",
      "        If the model is an encoder-decoder model (`model.config.is_encoder_decoder=True`), the possible\n",
      "        [`~file_utils.ModelOutput`] types are:\n",
      "\n",
      "            - [`~generation_utils.TFGreedySearchEncoderDecoderOutput`],\n",
      "            - [`~generation_utils.TFSampleEncoderDecoderOutput`],\n",
      "            - [`~generation_utils.TFBeamSearchEncoderDecoderOutput`],\n",
      "            - [`~generation_utils.TFBeamSampleEncoderDecoderOutput`]\n",
      "\n",
      "Examples:\n",
      "\n",
      "```python\n",
      "tokenizer = AutoTokenizer.from_pretrained(\"distilgpt2\")  # Initialize tokenizer\n",
      "model = TFAutoModelWithLMHead.from_pretrained(\n",
      "    \"distilgpt2\"\n",
      ")  # Download model and configuration from huggingface.co and cache.\n",
      "outputs = model.generate(max_length=40)  # do greedy decoding\n",
      "print(f\"Generated: {tokenizer.decode(outputs[0], skip_special_tokens=True)}\")\n",
      "\n",
      "tokenizer = AutoTokenizer.from_pretrained(\"openai-gpt\")  # Initialize tokenizer\n",
      "model = TFAutoModelWithLMHead.from_pretrained(\n",
      "    \"openai-gpt\"\n",
      ")  # Download model and configuration from huggingface.co and cache.\n",
      "input_context = \"The dog\"\n",
      "input_ids = tokenizer.encode(input_context, return_tensors=\"tf\")  # encode input context\n",
      "outputs = model.generate(\n",
      "    input_ids=input_ids, num_beams=5, num_return_sequences=3, temperature=1.5\n",
      ")  # generate 3 independent sequences using beam search decoding (5 beams) with sampling from initial context 'The dog'\n",
      "for i in range(3):  #  3 output sequences were generated\n",
      "    print(f\"Generated {i}: {tokenizer.decode(outputs[i], skip_special_tokens=True)}\")\n",
      "\n",
      "tokenizer = AutoTokenizer.from_pretrained(\"distilgpt2\")  # Initialize tokenizer\n",
      "model = TFAutoModelWithLMHead.from_pretrained(\n",
      "    \"distilgpt2\"\n",
      ")  # Download model and configuration from huggingface.co and cache.\n",
      "input_context = \"The dog\"\n",
      "input_ids = tokenizer.encode(input_context, return_tensors=\"tf\")  # encode input context\n",
      "outputs = model.generate(\n",
      "    input_ids=input_ids, max_length=40, temperature=0.7, num_return_sequences=3, do_sample=True\n",
      ")  # generate 3 candidates using sampling\n",
      "for i in range(3):  #  3 output sequences were generated\n",
      "    print(f\"Generated {i}: {tokenizer.decode(outputs[i], skip_special_tokens=True)}\")\n",
      "\n",
      "tokenizer = AutoTokenizer.from_pretrained(\"ctrl\")  # Initialize tokenizer\n",
      "model = TFAutoModelWithLMHead.from_pretrained(\n",
      "    \"ctrl\"\n",
      ")  # Download model and configuration from huggingface.co and cache.\n",
      "input_context = \"Legal My neighbor is\"  # \"Legal\" is one of the control codes for ctrl\n",
      "input_ids = tokenizer.encode(input_context, return_tensors=\"tf\")  # encode input context\n",
      "outputs = model.generate(\n",
      "    input_ids=input_ids, max_length=50, temperature=0.7, repetition_penalty=1.2\n",
      ")  # generate sequences\n",
      "print(f\"Generated: {tokenizer.decode(outputs[0], skip_special_tokens=True)}\")\n",
      "\n",
      "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")  # Initialize tokenizer\n",
      "model = TFAutoModelWithLMHead.from_pretrained(\n",
      "    \"gpt2\"\n",
      ")  # Download model and configuration from huggingface.co and cache.\n",
      "input_context = \"My cute dog\"\n",
      "bad_words_ids = [\n",
      "    tokenizer.encode(bad_word, add_prefix_space=True) for bad_word in [\"idiot\", \"stupid\", \"shut up\"]\n",
      "]\n",
      "input_ids = tokenizer.encode(input_context, return_tensors=\"tf\")  # encode input context\n",
      "outputs = model.generate(\n",
      "    input_ids=input_ids, max_length=100, do_sample=True, bad_words_ids=bad_words_ids\n",
      ")  # generate sequences without allowing bad_words to be generated\n",
      "```\n",
      "\u001b[0;31mFile:\u001b[0m      ~/.pyenv/versions/3.8.5/envs/transformers/lib/python3.8/site-packages/transformers/generation_tf_utils.py\n",
      "\u001b[0;31mType:\u001b[0m      method\n"
     ]
    }
   ],
   "source": [
    "?model.generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "bd65e91492a31472280b6a93b5564b20a5f87dac1fc0711a990b76f8ad770e22"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('transformers': pyenv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
