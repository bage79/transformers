{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://towardsdatascience.com/why-are-there-so-many-tokenization-methods-for-transformers-a340e493b3a8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello world!\n",
      "['hello world!', 'hello earth and mars!']\n"
     ]
    }
   ],
   "source": [
    "text = 'hello world!'\n",
    "text_list = ['hello world!', 'hello earth and mars!']\n",
    "print(text)\n",
    "print(text_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BartTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.models.bart.tokenization_bart.BartTokenizer'>\n",
      "pad: <pad> 1\n",
      "unk: <unk> 3\n",
      "mask: <mask> 50264\n",
      "bos: <s> 0\n",
      "eos: </s> 2\n",
      "cls: <s> 0\n",
      "sep: </s> 2\n"
     ]
    }
   ],
   "source": [
    "from transformers import BartTokenizer\n",
    "\n",
    "tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-large\")\n",
    "print(type(tokenizer))\n",
    "print(\"pad:\", tokenizer.pad_token, tokenizer.pad_token_id)\n",
    "print(\"unk:\", tokenizer.unk_token, tokenizer.unk_token_id)\n",
    "print(\"mask:\", tokenizer.mask_token, tokenizer.mask_token_id)\n",
    "print(\"bos:\", tokenizer.bos_token, tokenizer.bos_token_id)\n",
    "print(\"eos:\", tokenizer.eos_token, tokenizer.eos_token_id)\n",
    "print(\"cls:\", tokenizer.cls_token, tokenizer.cls_token_id)\n",
    "print(\"sep:\", tokenizer.sep_token, tokenizer.sep_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', 'Ġworld', '!']\n",
      "hello world!\n",
      "[42891, 232, 328]\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer.tokenize(text)   # string -> tokens\n",
    "print(tokenizer.tokenize(text))\n",
    "print(tokenizer.convert_tokens_to_string(tokens)) # tokens -> string\n",
    "print(tokenizer.convert_tokens_to_ids(tokens))    # tokens -> ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 42891, 232, 328, 2]\n",
      "{'input_ids': [0, 42891, 232, 328, 2], 'attention_mask': [1, 1, 1, 1, 1]}\n",
      "['<s>', 'hello', 'Ġworld', '!', '</s>']\n"
     ]
    }
   ],
   "source": [
    "ids = tokenizer.encode(text)\n",
    "print(tokenizer.encode(text)) # string -> tokens -> ids\n",
    "print(tokenizer.encode_plus(text)) # string -> tokens -> ids + more\n",
    "print(tokenizer.convert_ids_to_tokens(ids)) # ids -> tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pad: <pad> 1\n",
      "dict_keys(['input_ids', 'attention_mask'])\n",
      "input_ids tensor([[    0, 42891,   232,   328,     2,     1,     1,     1,     1,     1]])\n",
      "attention_mask tensor([[1, 1, 1, 1, 1, 0, 0, 0, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "encoded = tokenizer.encode_plus(text, max_length=10, padding='max_length', return_tensors='pt')\n",
    "print(\"pad:\", tokenizer.pad_token, tokenizer.pad_token_id)\n",
    "print(encoded.keys())\n",
    "for k in encoded:\n",
    "    print(k, encoded[k])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using bos_token, but it is not set yet.\n",
      "Using eos_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.models.bert.tokenization_bert.BertTokenizer'>\n",
      "pad: [PAD] 0\n",
      "unk: [UNK] 100\n",
      "mask: [MASK] 103\n",
      "bos: None None\n",
      "eos: None None\n",
      "cls: [CLS] 101\n",
      "sep: [SEP] 102\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "print(type(tokenizer))\n",
    "print(\"pad:\", tokenizer.pad_token, tokenizer.pad_token_id)\n",
    "print(\"unk:\", tokenizer.unk_token, tokenizer.unk_token_id)\n",
    "print(\"mask:\", tokenizer.mask_token, tokenizer.mask_token_id)\n",
    "print(\"bos:\", tokenizer.bos_token, tokenizer.bos_token_id)\n",
    "print(\"eos:\", tokenizer.eos_token, tokenizer.eos_token_id)\n",
    "print(\"cls:\", tokenizer.cls_token, tokenizer.cls_token_id)\n",
    "print(\"sep:\", tokenizer.sep_token, tokenizer.sep_token_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 7592, 2088, 999, 102]\n",
      "{'input_ids': [101, 7592, 2088, 999, 102], 'token_type_ids': [0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1]}\n",
      "['[CLS]', 'hello', 'world', '!', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "ids = tokenizer.encode(text)\n",
    "print(tokenizer.encode(text)) # string -> tokens -> ids\n",
    "print(tokenizer.encode_plus(text)) # string -> tokens -> ids + more\n",
    "print(tokenizer.convert_ids_to_tokens(ids)) # ids -> tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 101, 7592, 2088,  999,  102,    0,    0,    0,    0,    0]])\n",
      "torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "ids = tokenizer.encode(text, max_length=10, padding='max_length', return_tensors='pt')\n",
    "print(ids)\n",
    "print(ids.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])\n",
      "input_ids tensor([[ 101, 7592, 2088,  999,  102,    0,    0,    0,    0,    0]])\n",
      "token_type_ids tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "attention_mask tensor([[1, 1, 1, 1, 1, 0, 0, 0, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "encoded = tokenizer.encode_plus(text, max_length=10, padding='max_length', return_tensors='pt')\n",
    "print(encoded.keys())\n",
    "for k in encoded:\n",
    "    print(k, encoded[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])\n",
      "input_ids tensor([[ 101, 7592, 2088,  999,  102,    0,    0,    0,    0,    0],\n",
      "        [ 101, 7592, 3011, 1998, 7733,  999,  102,    0,    0,    0]])\n",
      "token_type_ids tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "attention_mask tensor([[1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 0, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "encoded = tokenizer.batch_encode_plus(text_list, max_length=10, padding='max_length', return_tensors='pt')\n",
    "print(encoded.keys())\n",
    "for k in encoded:\n",
    "    print(k, encoded[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 7592, 2088, 999, 102], 'token_type_ids': [0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1]}\n",
      "{'input_ids': [[101, 7592, 2088, 999, 102], [101, 7592, 3011, 1998, 7733, 999, 102]], 'token_type_ids': [[0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1]]}\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer(text))  # = encode_plus()\n",
    "print(tokenizer(text_list)) # = batch_encode_plus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "bd65e91492a31472280b6a93b5564b20a5f87dac1fc0711a990b76f8ad770e22"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('transformers': pyenv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
